# Transformer networks for Universal Multilingual Neural Machine Translation

# As implemented in the paper "Very Deep Self-Attention Networks for End-to-End Speech Recognition" https://arxiv.org/abs/1904.13377

----------------------------

# DATA 

(to be uploaded). Note that after the paper is published, we managed to get better scores than the paper using BPE instead of character units.

# TRAINING STEPS
(to be added)

## Data preparation


## Converting data to tensors


## Training


## Decoding 



